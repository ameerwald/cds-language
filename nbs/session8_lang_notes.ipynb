{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Language modelling with RNNs (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 12:49:21.780595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # return vocabulary in the text if not part of string.punctuation and make lowercase \n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # make sure utf8 encoding \n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, \n",
    "                        10, \n",
    "                        input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1)) # when determining weights during training, drop 10% of the weights - makes the model learn better, makes it a bit more difficult so it doesn't overfit as easily \n",
    "   \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len): # seed_text like a prompt we give it \n",
    "    for _ in range(next_words): # for however many in the range of next words, like 5 \n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], \n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list),\n",
    "                                            axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"..\", \"..\", \"431868\", \"news_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to load the data one at a time and append *only* the headlines to our list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(data_dir + \"/\" + filename)\n",
    "        all_headlines.extend(list(article_df[\"headline\"].values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean up a little bit and see how many data points we have.\n",
    "\n",
    "If called unknown, removed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a small amount of data which is why it doesn't work as well. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call out ```clean_text()``` function and then inspect the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my beijing the sacred city',\n",
       " '6 million riders a day 1930s technology',\n",
       " 'seeking a crossborder conference',\n",
       " 'questions for despite the yuck factor leeches are big in russian medicine',\n",
       " 'who is a criminal',\n",
       " 'an antidote to europes populism',\n",
       " 'the cost of a speech',\n",
       " 'degradation of the language',\n",
       " 'on the power of being awful',\n",
       " 'trump garbles pitch on a revised health bill']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the processing steps - stripping punctuation makes grammatically incorrect and can be problematic later "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We're then going to tokenize our data, using the ```Tokenizer()``` class from ```TensorFlow```, about which you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "We then use the ```get_sequence_of_tokens()``` function we defined above, which turns every text into a sequence of tokens based on the vocabulary from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1 # '+1' needs a way to index words it doesn't know; creates an arbitrary token, called an out of vocabulary token (oov)\n",
    "# looks like '<unk>' so every token it doesn't recognize, labels it this way so it can deal with unknown words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51770"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sequences = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10]\n",
    "len(inp_sequences) # have more now tha nthe 8000 whatever headlines, it returns lots of the same documents with varying n_gram lengths\n",
    "# can see first and second word, then first through third words and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'on': 8,\n",
       " 'is': 9,\n",
       " 'trump': 10,\n",
       " 'with': 11,\n",
       " 'new': 12,\n",
       " 'at': 13,\n",
       " 'how': 14,\n",
       " 'what': 15,\n",
       " 'you': 16,\n",
       " 'an': 17,\n",
       " 'from': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'trumps': 21,\n",
       " 'its': 22,\n",
       " 'your': 23,\n",
       " 'are': 24,\n",
       " 'be': 25,\n",
       " 'not': 26,\n",
       " 'us': 27,\n",
       " 'season': 28,\n",
       " 'that': 29,\n",
       " 'by': 30,\n",
       " 'about': 31,\n",
       " 'but': 32,\n",
       " 'can': 33,\n",
       " 'episode': 34,\n",
       " 'do': 35,\n",
       " 'this': 36,\n",
       " 'when': 37,\n",
       " 'york': 38,\n",
       " 'up': 39,\n",
       " 'over': 40,\n",
       " 'why': 41,\n",
       " 'no': 42,\n",
       " 'i': 43,\n",
       " 'out': 44,\n",
       " 'more': 45,\n",
       " 'my': 46,\n",
       " 'after': 47,\n",
       " 'will': 48,\n",
       " 'may': 49,\n",
       " 'we': 50,\n",
       " 'or': 51,\n",
       " 'war': 52,\n",
       " 'who': 53,\n",
       " 'his': 54,\n",
       " 'health': 55,\n",
       " 'teaching': 56,\n",
       " 'questions': 57,\n",
       " 'now': 58,\n",
       " 'president': 59,\n",
       " 'was': 60,\n",
       " 'one': 61,\n",
       " 'house': 62,\n",
       " 'get': 63,\n",
       " 'today': 64,\n",
       " 'have': 65,\n",
       " 'should': 66,\n",
       " 'into': 67,\n",
       " 'home': 68,\n",
       " 'all': 69,\n",
       " 'dont': 70,\n",
       " 'life': 71,\n",
       " 'our': 72,\n",
       " 'has': 73,\n",
       " 'plan': 74,\n",
       " 'good': 75,\n",
       " 'first': 76,\n",
       " 'gop': 77,\n",
       " '1': 78,\n",
       " 'says': 79,\n",
       " 'like': 80,\n",
       " 'white': 81,\n",
       " 'he': 82,\n",
       " '2': 83,\n",
       " 'too': 84,\n",
       " 'trade': 85,\n",
       " 'mr': 86,\n",
       " 'world': 87,\n",
       " 'big': 88,\n",
       " 'women': 89,\n",
       " 'love': 90,\n",
       " 'back': 91,\n",
       " '3': 92,\n",
       " 'right': 93,\n",
       " 'they': 94,\n",
       " 'north': 95,\n",
       " 'recap': 96,\n",
       " 'so': 97,\n",
       " 'their': 98,\n",
       " 'time': 99,\n",
       " 'just': 100,\n",
       " 'race': 101,\n",
       " 'her': 102,\n",
       " 'if': 103,\n",
       " 'russia': 104,\n",
       " 'black': 105,\n",
       " 'going': 106,\n",
       " 'better': 107,\n",
       " 'america': 108,\n",
       " 'go': 109,\n",
       " 'times': 110,\n",
       " 'donald': 111,\n",
       " 'china': 112,\n",
       " 'care': 113,\n",
       " 'way': 114,\n",
       " 'where': 115,\n",
       " 'help': 116,\n",
       " 'still': 117,\n",
       " 'say': 118,\n",
       " 'could': 119,\n",
       " 'city': 120,\n",
       " 'whats': 121,\n",
       " '6': 122,\n",
       " 'democrats': 123,\n",
       " 'end': 124,\n",
       " 'day': 125,\n",
       " 'power': 126,\n",
       " 'make': 127,\n",
       " 'variety': 128,\n",
       " 'activities': 129,\n",
       " 'man': 130,\n",
       " 'off': 131,\n",
       " 'korea': 132,\n",
       " 'news': 133,\n",
       " 'people': 134,\n",
       " 'rules': 135,\n",
       " 'tax': 136,\n",
       " 'some': 137,\n",
       " 'than': 138,\n",
       " 'own': 139,\n",
       " 'heart': 140,\n",
       " 'fight': 141,\n",
       " 'bill': 142,\n",
       " 'great': 143,\n",
       " '5': 144,\n",
       " 'two': 145,\n",
       " 'me': 146,\n",
       " 'work': 147,\n",
       " 'change': 148,\n",
       " 'vs': 149,\n",
       " 'political': 150,\n",
       " 'syria': 151,\n",
       " 'state': 152,\n",
       " 'bad': 153,\n",
       " 'court': 154,\n",
       " 'dead': 155,\n",
       " 'gun': 156,\n",
       " 'republicans': 157,\n",
       " 'again': 158,\n",
       " 'next': 159,\n",
       " 'years': 160,\n",
       " 'real': 161,\n",
       " 'another': 162,\n",
       " 'american': 163,\n",
       " 'does': 164,\n",
       " 'against': 165,\n",
       " 'deal': 166,\n",
       " 'past': 167,\n",
       " 'year': 168,\n",
       " 'take': 169,\n",
       " 'family': 170,\n",
       " 'food': 171,\n",
       " 'little': 172,\n",
       " 'here': 173,\n",
       " '2017': 174,\n",
       " 'would': 175,\n",
       " 'death': 176,\n",
       " 'want': 177,\n",
       " 'case': 178,\n",
       " 'takes': 179,\n",
       " 'chief': 180,\n",
       " 'obama': 181,\n",
       " 'school': 182,\n",
       " 'save': 183,\n",
       " 'old': 184,\n",
       " 'long': 185,\n",
       " 'then': 186,\n",
       " 'law': 187,\n",
       " 'money': 188,\n",
       " 'americans': 189,\n",
       " 'crisis': 190,\n",
       " 'talk': 191,\n",
       " 'picture': 192,\n",
       " 'call': 193,\n",
       " 'need': 194,\n",
       " 'student': 195,\n",
       " 'mind': 196,\n",
       " 'policy': 197,\n",
       " 'children': 198,\n",
       " 'vote': 199,\n",
       " 'behind': 200,\n",
       " 'men': 201,\n",
       " 'police': 202,\n",
       " 'battle': 203,\n",
       " 'future': 204,\n",
       " 'story': 205,\n",
       " 'facebook': 206,\n",
       " 'place': 207,\n",
       " 'under': 208,\n",
       " 'justice': 209,\n",
       " 'history': 210,\n",
       " 'before': 211,\n",
       " 'being': 212,\n",
       " 'wall': 213,\n",
       " 'see': 214,\n",
       " 'pay': 215,\n",
       " 'climate': 216,\n",
       " 'cant': 217,\n",
       " 'party': 218,\n",
       " 'politics': 219,\n",
       " 'homes': 220,\n",
       " 'young': 221,\n",
       " 'really': 222,\n",
       " 'risk': 223,\n",
       " 'down': 224,\n",
       " 'wont': 225,\n",
       " 'attack': 226,\n",
       " 'were': 227,\n",
       " 'think': 228,\n",
       " 'tariffs': 229,\n",
       " 'art': 230,\n",
       " 'students': 231,\n",
       " 'rise': 232,\n",
       " 'high': 233,\n",
       " 'college': 234,\n",
       " 'south': 235,\n",
       " 'eat': 236,\n",
       " 'kids': 237,\n",
       " 'immigration': 238,\n",
       " 'win': 239,\n",
       " 'keep': 240,\n",
       " 'sex': 241,\n",
       " 'leader': 242,\n",
       " 'fear': 243,\n",
       " 'republican': 244,\n",
       " 'there': 245,\n",
       " 'states': 246,\n",
       " '7': 247,\n",
       " '2018': 248,\n",
       " 'march': 249,\n",
       " 'russian': 250,\n",
       " 'without': 251,\n",
       " 'much': 252,\n",
       " 'best': 253,\n",
       " 'parents': 254,\n",
       " 'left': 255,\n",
       " 'them': 256,\n",
       " 'child': 257,\n",
       " 'california': 258,\n",
       " 'comey': 259,\n",
       " 'taking': 260,\n",
       " 'security': 261,\n",
       " 'free': 262,\n",
       " 'ban': 263,\n",
       " 'age': 264,\n",
       " 'living': 265,\n",
       " 'fix': 266,\n",
       " 'judge': 267,\n",
       " 'obamacare': 268,\n",
       " 'most': 269,\n",
       " 'while': 270,\n",
       " 'book': 271,\n",
       " 'making': 272,\n",
       " 'office': 273,\n",
       " 'border': 274,\n",
       " 'test': 275,\n",
       " 'schools': 276,\n",
       " 'subway': 277,\n",
       " 'live': 278,\n",
       " 'secret': 279,\n",
       " 'other': 280,\n",
       " 'top': 281,\n",
       " 'isnt': 282,\n",
       " 'last': 283,\n",
       " 'did': 284,\n",
       " 'french': 285,\n",
       " 'play': 286,\n",
       " 'only': 287,\n",
       " 'problem': 288,\n",
       " 'cancer': 289,\n",
       " 'open': 290,\n",
       " 'start': 291,\n",
       " 'game': 292,\n",
       " 'him': 293,\n",
       " 'fire': 294,\n",
       " 'words': 295,\n",
       " 'brooklyn': 296,\n",
       " 'nuclear': 297,\n",
       " 'control': 298,\n",
       " 'vietnam': 299,\n",
       " 'lead': 300,\n",
       " 'night': 301,\n",
       " 'close': 302,\n",
       " 'let': 303,\n",
       " 'use': 304,\n",
       " 'friday': 305,\n",
       " 'star': 306,\n",
       " 'rights': 307,\n",
       " '8': 308,\n",
       " 'inquiry': 309,\n",
       " 'know': 310,\n",
       " 'find': 311,\n",
       " 'metoo': 312,\n",
       " 'even': 313,\n",
       " '4': 314,\n",
       " 'mindful': 315,\n",
       " 'debate': 316,\n",
       " 'election': 317,\n",
       " 'many': 318,\n",
       " 'presidents': 319,\n",
       " 'senate': 320,\n",
       " 'act': 321,\n",
       " 'lost': 322,\n",
       " 'social': 323,\n",
       " 'teenagers': 324,\n",
       " 'fbi': 325,\n",
       " 'congress': 326,\n",
       " 'dies': 327,\n",
       " 'gets': 328,\n",
       " 'budget': 329,\n",
       " 'these': 330,\n",
       " 'million': 331,\n",
       " 'faces': 332,\n",
       " 'billions': 333,\n",
       " 'less': 334,\n",
       " 'missing': 335,\n",
       " 'ask': 336,\n",
       " 'face': 337,\n",
       " 'washington': 338,\n",
       " 'stand': 339,\n",
       " 'said': 340,\n",
       " 'acrostic': 341,\n",
       " 'teachers': 342,\n",
       " 'three': 343,\n",
       " 'mueller': 344,\n",
       " 'stop': 345,\n",
       " 'epa': 346,\n",
       " 'britain': 347,\n",
       " 'job': 348,\n",
       " 'getting': 349,\n",
       " 'media': 350,\n",
       " 'street': 351,\n",
       " 'door': 352,\n",
       " 'woman': 353,\n",
       " 'tale': 354,\n",
       " 'travel': 355,\n",
       " 'puzzle': 356,\n",
       " 'chinese': 357,\n",
       " 'hope': 358,\n",
       " 'lesson': 359,\n",
       " 'side': 360,\n",
       " 'era': 361,\n",
       " 'democracy': 362,\n",
       " 'global': 363,\n",
       " 'military': 364,\n",
       " 'voters': 365,\n",
       " 'truth': 366,\n",
       " 'wants': 367,\n",
       " 'talks': 368,\n",
       " 'yes': 369,\n",
       " 'supreme': 370,\n",
       " 'risks': 371,\n",
       " 'come': 372,\n",
       " 'looking': 373,\n",
       " 'im': 374,\n",
       " 'wrong': 375,\n",
       " 'mailbag': 376,\n",
       " 'market': 377,\n",
       " 'far': 378,\n",
       " 'data': 379,\n",
       " 'look': 380,\n",
       " 'plans': 381,\n",
       " 'walking': 382,\n",
       " 'favorite': 383,\n",
       " 'cuts': 384,\n",
       " 'role': 385,\n",
       " 'king': 386,\n",
       " 'things': 387,\n",
       " 'ready': 388,\n",
       " 'presidency': 389,\n",
       " 'safety': 390,\n",
       " 'tell': 391,\n",
       " 'action': 392,\n",
       " 'country': 393,\n",
       " '10': 394,\n",
       " 'needs': 395,\n",
       " 'west': 396,\n",
       " 'second': 397,\n",
       " 'moment': 398,\n",
       " 'youre': 399,\n",
       " 'made': 400,\n",
       " 'jobs': 401,\n",
       " 'finding': 402,\n",
       " 'move': 403,\n",
       " 'turn': 404,\n",
       " 'break': 405,\n",
       " 'isis': 406,\n",
       " 'building': 407,\n",
       " 'trial': 408,\n",
       " 'former': 409,\n",
       " 'across': 410,\n",
       " 'fake': 411,\n",
       " 'cut': 412,\n",
       " 'dream': 413,\n",
       " 'days': 414,\n",
       " 'americas': 415,\n",
       " 'science': 416,\n",
       " 'shutdown': 417,\n",
       " 'putin': 418,\n",
       " 'team': 419,\n",
       " 'near': 420,\n",
       " 'france': 421,\n",
       " 'de': 422,\n",
       " 'learning': 423,\n",
       " 'early': 424,\n",
       " 'aid': 425,\n",
       " 'cuomo': 426,\n",
       " 'fast': 427,\n",
       " 'try': 428,\n",
       " 'air': 429,\n",
       " 'abuse': 430,\n",
       " 'stars': 431,\n",
       " 'dept': 432,\n",
       " 'leaders': 433,\n",
       " 'found': 434,\n",
       " 'friends': 435,\n",
       " 'put': 436,\n",
       " 'coming': 437,\n",
       " 'goes': 438,\n",
       " 'voice': 439,\n",
       " 'readers': 440,\n",
       " 'hate': 441,\n",
       " 'learn': 442,\n",
       " 'red': 443,\n",
       " 'shows': 444,\n",
       " 'winter': 445,\n",
       " 'path': 446,\n",
       " 'doctor': 447,\n",
       " 'girls': 448,\n",
       " 'must': 449,\n",
       " 'drug': 450,\n",
       " 'national': 451,\n",
       " 'challenge': 452,\n",
       " 'pain': 453,\n",
       " 'hard': 454,\n",
       " 'tech': 455,\n",
       " 'feel': 456,\n",
       " 'blood': 457,\n",
       " 'got': 458,\n",
       " 'united': 459,\n",
       " 'run': 460,\n",
       " 'sale': 461,\n",
       " 'price': 462,\n",
       " 'slow': 463,\n",
       " 'john': 464,\n",
       " 'sessions': 465,\n",
       " 'pick': 466,\n",
       " 'set': 467,\n",
       " 'makes': 468,\n",
       " 'give': 469,\n",
       " 'week': 470,\n",
       " 'meet': 471,\n",
       " 'education': 472,\n",
       " 'threat': 473,\n",
       " 'bannon': 474,\n",
       " 'speech': 475,\n",
       " 'claims': 476,\n",
       " 'james': 477,\n",
       " 'crossword': 478,\n",
       " 'search': 479,\n",
       " 'heres': 480,\n",
       " 'show': 481,\n",
       " 'support': 482,\n",
       " 'meeting': 483,\n",
       " 'business': 484,\n",
       " 'sports': 485,\n",
       " 'stephen': 486,\n",
       " 'tells': 487,\n",
       " 'had': 488,\n",
       " 'well': 489,\n",
       " 'town': 490,\n",
       " 'force': 491,\n",
       " 'thats': 492,\n",
       " 'inside': 493,\n",
       " 'question': 494,\n",
       " 'legal': 495,\n",
       " 'room': 496,\n",
       " 'comes': 497,\n",
       " 'body': 498,\n",
       " 'lives': 499,\n",
       " 'looks': 500,\n",
       " 'officials': 501,\n",
       " 'becomes': 502,\n",
       " 'trip': 503,\n",
       " 'loss': 504,\n",
       " 'pressure': 505,\n",
       " 'fighting': 506,\n",
       " 'florida': 507,\n",
       " 'silence': 508,\n",
       " 'through': 509,\n",
       " 'rising': 510,\n",
       " 'memory': 511,\n",
       " 'she': 512,\n",
       " 'away': 513,\n",
       " 'trust': 514,\n",
       " 'idea': 515,\n",
       " 'small': 516,\n",
       " 'guns': 517,\n",
       " 'leave': 518,\n",
       " 'allies': 519,\n",
       " 'been': 520,\n",
       " 'doesnt': 521,\n",
       " 'might': 522,\n",
       " 'choice': 523,\n",
       " 'attacks': 524,\n",
       " 'island': 525,\n",
       " 'doctors': 526,\n",
       " 'tv': 527,\n",
       " 'reality': 528,\n",
       " 'matter': 529,\n",
       " 'met': 530,\n",
       " 'la': 531,\n",
       " 'fall': 532,\n",
       " 'modern': 533,\n",
       " 'road': 534,\n",
       " 'hes': 535,\n",
       " 'syrian': 536,\n",
       " 'meets': 537,\n",
       " 'music': 538,\n",
       " 'didnt': 539,\n",
       " 'baby': 540,\n",
       " 'tied': 541,\n",
       " 'raise': 542,\n",
       " 'tests': 543,\n",
       " 'israel': 544,\n",
       " 'hot': 545,\n",
       " 'workers': 546,\n",
       " 'jimmy': 547,\n",
       " 'governor': 548,\n",
       " 'ways': 549,\n",
       " 'report': 550,\n",
       " 'wins': 551,\n",
       " 'privacy': 552,\n",
       " 'economy': 553,\n",
       " 'advice': 554,\n",
       " 'feud': 555,\n",
       " 'land': 556,\n",
       " 'video': 557,\n",
       " 'crash': 558,\n",
       " 'brain': 559,\n",
       " 'view': 560,\n",
       " 'others': 561,\n",
       " 'texas': 562,\n",
       " 'something': 563,\n",
       " 'worth': 564,\n",
       " 'bar': 565,\n",
       " 'word': 566,\n",
       " 'once': 567,\n",
       " 'europe': 568,\n",
       " 'repeal': 569,\n",
       " 'broken': 570,\n",
       " 'mother': 571,\n",
       " 'beat': 572,\n",
       " 'easy': 573,\n",
       " 'fired': 574,\n",
       " 'dear': 575,\n",
       " 'economic': 576,\n",
       " 'safe': 577,\n",
       " 'thing': 578,\n",
       " 'aide': 579,\n",
       " 'fish': 580,\n",
       " 'fashion': 581,\n",
       " 'nation': 582,\n",
       " 'contest': 583,\n",
       " 'push': 584,\n",
       " 'match': 585,\n",
       " 'watch': 586,\n",
       " 'womens': 587,\n",
       " 'turns': 588,\n",
       " 'target': 589,\n",
       " 'weight': 590,\n",
       " 'paid': 591,\n",
       " 'lets': 592,\n",
       " 'pregnancy': 593,\n",
       " 'warm': 594,\n",
       " 'hollywood': 595,\n",
       " 'any': 596,\n",
       " 'freedom': 597,\n",
       " 'cold': 598,\n",
       " 'spring': 599,\n",
       " 'success': 600,\n",
       " 'losing': 601,\n",
       " 'ever': 602,\n",
       " 'lose': 603,\n",
       " 'dr': 604,\n",
       " 'scandal': 605,\n",
       " 'homeland': 606,\n",
       " 'shooting': 607,\n",
       " '100': 608,\n",
       " 'abortion': 609,\n",
       " '9': 610,\n",
       " 'common': 611,\n",
       " 'teams': 612,\n",
       " 'drugs': 613,\n",
       " 'defense': 614,\n",
       " 'low': 615,\n",
       " 'fine': 616,\n",
       " 'courts': 617,\n",
       " 'finale': 618,\n",
       " 'human': 619,\n",
       " 'review': 620,\n",
       " 'leads': 621,\n",
       " 'yet': 622,\n",
       " 'water': 623,\n",
       " 'bring': 624,\n",
       " 'welcome': 625,\n",
       " 'blame': 626,\n",
       " 'between': 627,\n",
       " 'beyond': 628,\n",
       " 'both': 629,\n",
       " 'part': 630,\n",
       " 'russians': 631,\n",
       " 'killed': 632,\n",
       " 'happy': 633,\n",
       " 'oil': 634,\n",
       " 'simple': 635,\n",
       " 'spending': 636,\n",
       " 'ethics': 637,\n",
       " 'cities': 638,\n",
       " 'edge': 639,\n",
       " 'hall': 640,\n",
       " 'affair': 641,\n",
       " 'fans': 642,\n",
       " 'growth': 643,\n",
       " 'energy': 644,\n",
       " 'immigrants': 645,\n",
       " 'democratic': 646,\n",
       " 'manhattan': 647,\n",
       " 'grows': 648,\n",
       " 'longer': 649,\n",
       " 'chaos': 650,\n",
       " 'quiet': 651,\n",
       " 'killing': 652,\n",
       " 'hopes': 653,\n",
       " 'fears': 654,\n",
       " 'list': 655,\n",
       " 'different': 656,\n",
       " 'whos': 657,\n",
       " 'olympics': 658,\n",
       " 'key': 659,\n",
       " 'labor': 660,\n",
       " 'never': 661,\n",
       " 'ryan': 662,\n",
       " 'car': 663,\n",
       " 'medicine': 664,\n",
       " 'hit': 665,\n",
       " 'liberal': 666,\n",
       " 'blue': 667,\n",
       " 'fox': 668,\n",
       " 'raising': 669,\n",
       " 'return': 670,\n",
       " 'presidential': 671,\n",
       " 'center': 672,\n",
       " 'space': 673,\n",
       " 'study': 674,\n",
       " 'fargo': 675,\n",
       " 'ride': 676,\n",
       " 'theres': 677,\n",
       " 'lessons': 678,\n",
       " 'mean': 679,\n",
       " 'investors': 680,\n",
       " 'ends': 681,\n",
       " 'train': 682,\n",
       " 'wish': 683,\n",
       " 'birth': 684,\n",
       " 'paris': 685,\n",
       " 'sees': 686,\n",
       " 'charge': 687,\n",
       " 'turkey': 688,\n",
       " 'eye': 689,\n",
       " 'step': 690,\n",
       " 'system': 691,\n",
       " 'press': 692,\n",
       " 'millions': 693,\n",
       " 'government': 694,\n",
       " 'point': 695,\n",
       " '15': 696,\n",
       " 'limits': 697,\n",
       " 'special': 698,\n",
       " 'become': 699,\n",
       " 'turning': 700,\n",
       " 'short': 701,\n",
       " 'india': 702,\n",
       " 'chinas': 703,\n",
       " 'very': 704,\n",
       " 'sets': 705,\n",
       " 'four': 706,\n",
       " 'peace': 707,\n",
       " 'embrace': 708,\n",
       " 'line': 709,\n",
       " 'class': 710,\n",
       " 'decision': 711,\n",
       " 'familiar': 712,\n",
       " 'memo': 713,\n",
       " 'drag': 714,\n",
       " 'gone': 715,\n",
       " 'offer': 716,\n",
       " 'dreamers': 717,\n",
       " 'every': 718,\n",
       " 'speak': 719,\n",
       " 'light': 720,\n",
       " 'puts': 721,\n",
       " 'sick': 722,\n",
       " 'porn': 723,\n",
       " 'head': 724,\n",
       " 'valley': 725,\n",
       " 'ok': 726,\n",
       " 'few': 727,\n",
       " 'kind': 728,\n",
       " 'jail': 729,\n",
       " 'yorks': 730,\n",
       " 'aides': 731,\n",
       " 'premiere': 732,\n",
       " 'record': 733,\n",
       " 'older': 734,\n",
       " 'girl': 735,\n",
       " 'patients': 736,\n",
       " 'effect': 737,\n",
       " '13': 738,\n",
       " 'fury': 739,\n",
       " 'magic': 740,\n",
       " 'enough': 741,\n",
       " 'name': 742,\n",
       " 'add': 743,\n",
       " 'classic': 744,\n",
       " 'books': 745,\n",
       " 'phone': 746,\n",
       " 'promise': 747,\n",
       " 'jersey': 748,\n",
       " 'gives': 749,\n",
       " 'shift': 750,\n",
       " 'same': 751,\n",
       " 'secrets': 752,\n",
       " 'va': 753,\n",
       " 'stay': 754,\n",
       " 'amid': 755,\n",
       " 'thinks': 756,\n",
       " 'pet': 757,\n",
       " 'shot': 758,\n",
       " 'google': 759,\n",
       " 'everyone': 760,\n",
       " 'maybe': 761,\n",
       " 'deadly': 762,\n",
       " 'gave': 763,\n",
       " 'false': 764,\n",
       " 'walk': 765,\n",
       " 'always': 766,\n",
       " 'dog': 767,\n",
       " 'sea': 768,\n",
       " 'sleep': 769,\n",
       " '12': 770,\n",
       " 'hits': 771,\n",
       " 'believe': 772,\n",
       " 'taste': 773,\n",
       " 'kill': 774,\n",
       " 'deals': 775,\n",
       " 'kushner': 776,\n",
       " 'charges': 777,\n",
       " 'chance': 778,\n",
       " 'die': 779,\n",
       " 'trying': 780,\n",
       " 'course': 781,\n",
       " 'streets': 782,\n",
       " 'service': 783,\n",
       " 'alienist': 784,\n",
       " 'trouble': 785,\n",
       " 'robots': 786,\n",
       " 'rupauls': 787,\n",
       " 'gay': 788,\n",
       " 'super': 789,\n",
       " 'moves': 790,\n",
       " 'lot': 791,\n",
       " 'public': 792,\n",
       " 'protests': 793,\n",
       " 'noah': 794,\n",
       " 'fresh': 795,\n",
       " 'colbert': 796,\n",
       " 'guilty': 797,\n",
       " 'strikes': 798,\n",
       " 'iran': 799,\n",
       " 'toll': 800,\n",
       " 'fed': 801,\n",
       " 'storm': 802,\n",
       " 'details': 803,\n",
       " 'mexico': 804,\n",
       " 'teach': 805,\n",
       " 'silicon': 806,\n",
       " 'hours': 807,\n",
       " 'transgender': 808,\n",
       " 'online': 809,\n",
       " 'lies': 810,\n",
       " 'overlooked': 811,\n",
       " 'technology': 812,\n",
       " 'bright': 813,\n",
       " 'avoid': 814,\n",
       " 'calls': 815,\n",
       " 'chef': 816,\n",
       " 'industry': 817,\n",
       " 'reach': 818,\n",
       " 'votes': 819,\n",
       " 'pass': 820,\n",
       " 'diabetes': 821,\n",
       " 'trail': 822,\n",
       " 'crime': 823,\n",
       " 'disaster': 824,\n",
       " 'rate': 825,\n",
       " 'split': 826,\n",
       " 'southern': 827,\n",
       " 'federal': 828,\n",
       " 'wait': 829,\n",
       " 'lower': 830,\n",
       " 'finds': 831,\n",
       " 'person': 832,\n",
       " 'front': 833,\n",
       " 'text': 834,\n",
       " 'director': 835,\n",
       " 'which': 836,\n",
       " 'billion': 837,\n",
       " 'order': 838,\n",
       " 'intelligence': 839,\n",
       " 'equal': 840,\n",
       " 'calling': 841,\n",
       " 'used': 842,\n",
       " 'koreas': 843,\n",
       " 'colleges': 844,\n",
       " 'gap': 845,\n",
       " 'math': 846,\n",
       " 'clues': 847,\n",
       " 'took': 848,\n",
       " 'housing': 849,\n",
       " 'cover': 850,\n",
       " 'paul': 851,\n",
       " 'uber': 852,\n",
       " 'kitchen': 853,\n",
       " 'steps': 854,\n",
       " 'hell': 855,\n",
       " 'angry': 856,\n",
       " 'sweet': 857,\n",
       " 'box': 858,\n",
       " 'photos': 859,\n",
       " 'dirty': 860,\n",
       " 'arms': 861,\n",
       " 'nothing': 862,\n",
       " 'divide': 863,\n",
       " 'reading': 864,\n",
       " 'worse': 865,\n",
       " 'worst': 866,\n",
       " 'prison': 867,\n",
       " 'hero': 868,\n",
       " 'tune': 869,\n",
       " 'earth': 870,\n",
       " 'working': 871,\n",
       " 'sorry': 872,\n",
       " 'dogs': 873,\n",
       " 'canada': 874,\n",
       " 'artists': 875,\n",
       " 'jan': 876,\n",
       " 'using': 877,\n",
       " 'hands': 878,\n",
       " 'cash': 879,\n",
       " 'finally': 880,\n",
       " 'amazon': 881,\n",
       " 'harassment': 882,\n",
       " 'bowl': 883,\n",
       " 'soul': 884,\n",
       " 'assassination': 885,\n",
       " 'mayor': 886,\n",
       " 'clash': 887,\n",
       " 'exercise': 888,\n",
       " 'marijuana': 889,\n",
       " 'homeless': 890,\n",
       " 'hold': 891,\n",
       " 'trevor': 892,\n",
       " 'sound': 893,\n",
       " 'lie': 894,\n",
       " 'focus': 895,\n",
       " 'leaves': 896,\n",
       " 'foreign': 897,\n",
       " 'knows': 898,\n",
       " 'theyre': 899,\n",
       " 'youve': 900,\n",
       " 'tower': 901,\n",
       " 'strike': 902,\n",
       " 'returns': 903,\n",
       " 'games': 904,\n",
       " 'wonkish': 905,\n",
       " 'abroad': 906,\n",
       " 'heck': 907,\n",
       " 'legacy': 908,\n",
       " 'civil': 909,\n",
       " 'dying': 910,\n",
       " 'stress': 911,\n",
       " 'voting': 912,\n",
       " 'boss': 913,\n",
       " 'quiz': 914,\n",
       " 'seeking': 915,\n",
       " 'cost': 916,\n",
       " 'reporter': 917,\n",
       " 'cause': 918,\n",
       " 'full': 919,\n",
       " 'restaurant': 920,\n",
       " 'lines': 921,\n",
       " 'likely': 922,\n",
       " 'later': 923,\n",
       " 'clinton': 924,\n",
       " 'gender': 925,\n",
       " 'refugees': 926,\n",
       " 'driving': 927,\n",
       " 'senators': 928,\n",
       " 'empire': 929,\n",
       " 'station': 930,\n",
       " 'ivanka': 931,\n",
       " 'remember': 932,\n",
       " 'macron': 933,\n",
       " 'army': 934,\n",
       " 'secretary': 935,\n",
       " 'campaign': 936,\n",
       " 'coop': 937,\n",
       " 'ahead': 938,\n",
       " 'remain': 939,\n",
       " 'doing': 940,\n",
       " 'golden': 941,\n",
       " 'knew': 942,\n",
       " 'tiny': 943,\n",
       " 'reporting': 944,\n",
       " 'tries': 945,\n",
       " 'approach': 946,\n",
       " 'those': 947,\n",
       " 'read': 948,\n",
       " 'wild': 949,\n",
       " 'hear': 950,\n",
       " 'deep': 951,\n",
       " 'months': 952,\n",
       " 'since': 953,\n",
       " 'letter': 954,\n",
       " 'cooking': 955,\n",
       " 'shifts': 956,\n",
       " 'mothers': 957,\n",
       " 'lady': 958,\n",
       " 'boys': 959,\n",
       " 'signs': 960,\n",
       " 'flight': 961,\n",
       " 'madness': 962,\n",
       " 'toward': 963,\n",
       " 'major': 964,\n",
       " 'god': 965,\n",
       " 'terror': 966,\n",
       " 'blasio': 967,\n",
       " 'late': 968,\n",
       " 'stories': 969,\n",
       " 'led': 970,\n",
       " 'ideas': 971,\n",
       " 'winners': 972,\n",
       " 'falling': 973,\n",
       " 'internet': 974,\n",
       " 'missile': 975,\n",
       " 'came': 976,\n",
       " 'spy': 977,\n",
       " 'jeff': 978,\n",
       " 'running': 979,\n",
       " 'message': 980,\n",
       " 'vows': 981,\n",
       " 'assault': 982,\n",
       " 'issue': 983,\n",
       " 'ally': 984,\n",
       " 'murder': 985,\n",
       " 'park': 986,\n",
       " 'unlikely': 987,\n",
       " 'facing': 988,\n",
       " 'diet': 989,\n",
       " 'comments': 990,\n",
       " 'waiting': 991,\n",
       " 'opioids': 992,\n",
       " 'begins': 993,\n",
       " 'sanctions': 994,\n",
       " 'partisan': 995,\n",
       " 'families': 996,\n",
       " 'tree': 997,\n",
       " 'gold': 998,\n",
       " 'refugee': 999,\n",
       " 'nfl': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to *pad* our input sequences to make them all the same length. Adding 0's to make sure all the input strings are same length as the longest document. If the longest headline is 10 words, headlines of three words will be three numbers with seven 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len\n",
    "predictors # input vectors\n",
    "label # looks like one hot encoding vectors "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the ```create_model()``` function created above to initialize a model, telling the model the length of sequences and the total size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 10)            112650    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11265)             1137765   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,294,815\n",
      "Trainable params: 1,294,815\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 12:51:42.563648: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-24 12:51:42.565019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-24 12:51:42.566467: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is exactly the same as last week, but instead of document labels, we're fitting the model to predict next word.\n",
    "\n",
    "*NB!* This will take some time to train! It took me 35 minutes on UCloud 32xCPU.\n",
    "\n",
    "This model.fit function, runs based on previous as well. So if run the following chunk twice, it will run for 200 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "405/405 [==============================] - 24s 54ms/step - loss: 7.8858\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 7.4808\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 21s 51ms/step - loss: 7.3973\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 7.3198\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 7.2757\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 7.1382\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 7.0256\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.9065\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.7823\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 21s 51ms/step - loss: 6.6585\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.5362\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.4143\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.2965\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.1825\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 6.0657\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 5.9572\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 5.8483\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 5.7421\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 5.6396\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 5.5373\n",
      "Epoch 21/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 5.4372\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 5.3347\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 5.2411\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 5.1475\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 5.0531\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.9663\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 4.8817\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.7997\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.7190\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.6446\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.5681\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 4.4958\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 4.4289\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.3631\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.2995\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.2391\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 4.1801\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.1241\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.0681\n",
      "Epoch 40/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 4.0187\n",
      "Epoch 41/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 3.9666\n",
      "Epoch 42/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.9167\n",
      "Epoch 43/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.8711\n",
      "Epoch 44/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.8245\n",
      "Epoch 45/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.7812\n",
      "Epoch 46/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 3.7393\n",
      "Epoch 47/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.6958\n",
      "Epoch 48/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.6579\n",
      "Epoch 49/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.6153\n",
      "Epoch 50/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.5808\n",
      "Epoch 51/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.5409\n",
      "Epoch 52/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.5032\n",
      "Epoch 53/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.4694\n",
      "Epoch 54/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.4364\n",
      "Epoch 55/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.4040\n",
      "Epoch 56/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.3716\n",
      "Epoch 57/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.3385\n",
      "Epoch 58/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.3087\n",
      "Epoch 59/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.2741\n",
      "Epoch 60/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.2457\n",
      "Epoch 61/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 3.2180\n",
      "Epoch 62/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.1889\n",
      "Epoch 63/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.1600\n",
      "Epoch 64/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.1395\n",
      "Epoch 65/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.1117\n",
      "Epoch 66/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.0838\n",
      "Epoch 67/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.0567\n",
      "Epoch 68/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 3.0329\n",
      "Epoch 69/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 3.0138\n",
      "Epoch 70/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.9883\n",
      "Epoch 71/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 2.9616\n",
      "Epoch 72/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.9382\n",
      "Epoch 73/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.9192\n",
      "Epoch 74/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 2.8955\n",
      "Epoch 75/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.8813\n",
      "Epoch 76/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.8552\n",
      "Epoch 77/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.8354\n",
      "Epoch 78/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.8154\n",
      "Epoch 79/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.7935\n",
      "Epoch 80/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 2.7774\n",
      "Epoch 81/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.7583\n",
      "Epoch 82/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.7426\n",
      "Epoch 83/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.7214\n",
      "Epoch 84/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.7066\n",
      "Epoch 85/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.6854\n",
      "Epoch 86/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 2.6740\n",
      "Epoch 87/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.6523\n",
      "Epoch 88/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.6457\n",
      "Epoch 89/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 2.6199\n",
      "Epoch 90/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 2.6081\n",
      "Epoch 91/100\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 2.5957\n",
      "Epoch 92/100\n",
      "405/405 [==============================] - 22s 54ms/step - loss: 2.5784\n",
      "Epoch 93/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.5659\n",
      "Epoch 94/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 2.5498\n",
      "Epoch 95/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.5364\n",
      "Epoch 96/100\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 2.5232\n",
      "Epoch 97/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.5049\n",
      "Epoch 98/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.4967\n",
      "Epoch 99/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.4869\n",
      "Epoch 100/100\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 2.4761\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, # data like X_train and y_train\n",
    "                    label, # labels\n",
    "                    epochs=100, # arbitarily picked this number, with more epochs and time could get more accurate \n",
    "                    batch_size=128, # picked this size due to how long it takes \n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has trained, we can then use this to generate *new text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Danish Inventor Is Found Guilty In\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"danish\", 5, model, max_sequence_len)) # try word russia ; here 5 asks for 5 words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean and what can we use this for? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the embedding layer as a trainable parameter, we can instead using a *pretrained word embedding* model like ```word2vec```.\n",
    "\n",
    "In the following examples, we're using [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). These are trained a little differently from ```word2vec``` but they behave in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../../../431868/glove_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m path_to_glove_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m431868\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mglove_models\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m      3\u001b[0m embeddings_index \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_to_glove_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m         word, coefs \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(maxsplit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../../../431868/glove_models'"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = os.path.join(\"..\", \"..\", \"..\", \"431868\", \"glove_models\") \n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some variables that we're going to use later.\n",
    "\n",
    "With hits and misses, we're counting how many words in the corpus vocabulary have a corresponding GloVe embedding; misses are the words which appear in our vocabulary but which do not have a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer - notice that this is different\n",
    "    model.add(Embedding(\n",
    "            total_words,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=input_len)\n",
    "    )\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"china\", 30, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the assignment - one script to train the model and one to load the saved model and generate new text "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
